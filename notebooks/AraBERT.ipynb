{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03fe5db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "Wed Apr 21 05:33:41 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.102.04   Driver Version: 450.102.04   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN RTX           Off  | 00000000:18:00.0 Off |                  N/A |\n",
      "|100%   88C    P2   193W / 280W |  21876MiB / 24220MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN RTX           Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 41%   46C    P8     2W / 280W |      3MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN RTX           Off  | 00000000:86:00.0 Off |                  N/A |\n",
      "| 41%   34C    P8    15W / 280W |      3MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     32184      C   python                          21873MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"  # specify which GPU(s) to be used\n",
    "\n",
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    !nvidia-smi\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a91610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: optuna==2.3.0 in ./env/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: colorlog in ./env/lib/python3.7/site-packages (from optuna==2.3.0) (5.0.1)\n",
      "Requirement already satisfied: cmaes>=0.6.0 in ./env/lib/python3.7/site-packages (from optuna==2.3.0) (0.8.2)\n",
      "Requirement already satisfied: joblib in ./env/lib/python3.7/site-packages (from optuna==2.3.0) (1.0.1)\n",
      "Requirement already satisfied: scipy!=1.4.0 in ./env/lib/python3.7/site-packages (from optuna==2.3.0) (1.6.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in ./env/lib/python3.7/site-packages (from optuna==2.3.0) (1.4.9)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.7/site-packages (from optuna==2.3.0) (1.20.1)\n",
      "Requirement already satisfied: cliff in ./env/lib/python3.7/site-packages (from optuna==2.3.0) (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.7/site-packages (from optuna==2.3.0) (20.9)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.7/site-packages (from optuna==2.3.0) (4.60.0)\n",
      "Requirement already satisfied: alembic in ./env/lib/python3.7/site-packages (from optuna==2.3.0) (1.5.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./env/lib/python3.7/site-packages (from packaging>=20.0->optuna==2.3.0) (2.4.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./env/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata in ./env/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna==2.3.0) (3.10.1)\n",
      "Requirement already satisfied: python-editor>=0.3 in ./env/lib/python3.7/site-packages (from alembic->optuna==2.3.0) (1.0.4)\n",
      "Requirement already satisfied: Mako in ./env/lib/python3.7/site-packages (from alembic->optuna==2.3.0) (1.1.4)\n",
      "Requirement already satisfied: python-dateutil in ./env/lib/python3.7/site-packages (from alembic->optuna==2.3.0) (2.8.1)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in ./env/lib/python3.7/site-packages (from cliff->optuna==2.3.0) (1.5.0)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in ./env/lib/python3.7/site-packages (from cliff->optuna==2.3.0) (2.1.0)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in ./env/lib/python3.7/site-packages (from cliff->optuna==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: PyYAML>=3.12 in ./env/lib/python3.7/site-packages (from cliff->optuna==2.3.0) (5.3.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in ./env/lib/python3.7/site-packages (from cliff->optuna==2.3.0) (5.5.1)\n",
      "Requirement already satisfied: attrs>=16.3.0 in ./env/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (20.3.0)\n",
      "Requirement already satisfied: pyperclip>=1.6 in ./env/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (1.8.1)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in ./env/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (0.2.5)\n",
      "Requirement already satisfied: colorama>=0.3.7 in ./env/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (0.4.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./env/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==2.3.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in ./env/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==2.3.0) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in ./env/lib/python3.7/site-packages (from Mako->alembic->optuna==2.3.0) (1.1.1)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.7/site-packages (from python-dateutil->alembic->optuna==2.3.0) (1.15.0)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: transformers==4.2.1 in ./env/lib/python3.7/site-packages (4.2.1)\n",
      "Requirement already satisfied: sacremoses in ./env/lib/python3.7/site-packages (from transformers==4.2.1) (0.0.45)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.7/site-packages (from transformers==4.2.1) (1.20.1)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.7/site-packages (from transformers==4.2.1) (2.25.1)\n",
      "Requirement already satisfied: packaging in ./env/lib/python3.7/site-packages (from transformers==4.2.1) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./env/lib/python3.7/site-packages (from transformers==4.2.1) (2021.4.4)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in ./env/lib/python3.7/site-packages (from transformers==4.2.1) (0.9.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.7/site-packages (from transformers==4.2.1) (4.60.0)\n",
      "Requirement already satisfied: importlib-metadata in ./env/lib/python3.7/site-packages (from transformers==4.2.1) (3.10.1)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.7/site-packages (from transformers==4.2.1) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./env/lib/python3.7/site-packages (from importlib-metadata->transformers==4.2.1) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in ./env/lib/python3.7/site-packages (from importlib-metadata->transformers==4.2.1) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./env/lib/python3.7/site-packages (from packaging->transformers==4.2.1) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./env/lib/python3.7/site-packages (from requests->transformers==4.2.1) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./env/lib/python3.7/site-packages (from requests->transformers==4.2.1) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./env/lib/python3.7/site-packages (from requests->transformers==4.2.1) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.7/site-packages (from requests->transformers==4.2.1) (2020.12.5)\n",
      "Requirement already satisfied: joblib in ./env/lib/python3.7/site-packages (from sacremoses->transformers==4.2.1) (1.0.1)\n",
      "Requirement already satisfied: click in ./env/lib/python3.7/site-packages (from sacremoses->transformers==4.2.1) (7.1.2)\n",
      "Requirement already satisfied: six in ./env/lib/python3.7/site-packages (from sacremoses->transformers==4.2.1) (1.15.0)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: farasapy in ./env/lib/python3.7/site-packages (0.0.13)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.7/site-packages (from farasapy) (4.60.0)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.7/site-packages (from farasapy) (2.25.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./env/lib/python3.7/site-packages (from requests->farasapy) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./env/lib/python3.7/site-packages (from requests->farasapy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./env/lib/python3.7/site-packages (from requests->farasapy) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.7/site-packages (from requests->farasapy) (2020.12.5)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: pyarabic in ./env/lib/python3.7/site-packages (0.6.10)\n",
      "fatal: destination path 'arabert' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna==2.3.0\n",
    "!pip install transformers==4.2.1\n",
    "!pip install farasapy\n",
    "!pip install pyarabic\n",
    "!git clone https://github.com/aub-mind/arabert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "748e244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2c5738ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# TO-DO: Add ASAD Dataset\n",
    "import json\n",
    "RAW_DATA = True\n",
    "pd.options.mode.chained_assignment = None \n",
    "if RAW_DATA:\n",
    "    df_ASAD = pd.read_csv(\n",
    "        \"data/training_file.csv\", header=0\n",
    "    )\n",
    "    df_ASAD = df_ASAD[['Tweet_id','sentiment']]\n",
    "    df_ASAD.rename(columns = {'Tweet_id':'id', 'sentiment':'sentiment'}, inplace=True)\n",
    "   \n",
    "    # Replace all the tweet ids with text\n",
    "        # Read from JSON file\n",
    "    with open(\"data/training_file.json\") as jsonFile:\n",
    "        training_text = json.load(jsonFile)\n",
    "    textframe = pd.DataFrame.from_dict(training_text)\n",
    "    df_ASAD[\"sentiment\"]=df_ASAD[\"sentiment\"].astype(str)\n",
    "    df_ASAD[\"id\"] = df_ASAD[\"id\"].astype(str)\n",
    "    textframe[\"text\"] = textframe[\"text\"].astype(str)\n",
    "    textframe[\"id\"] = textframe[\"id\"].astype(str)\n",
    "    print(df_ASAD.columns)\n",
    "#     print(len(textframe[[\"id\"].unique() == len(textframe[\"id\"]))\n",
    "    df_ASAD = df_ASAD.merge(textframe, on = 'id', how=\"left\")\n",
    "#     df_ASAD[\"text\"] = textframe[textframe.id == df_ASAD.id][\"text\"]\n",
    "    #     print(training_text[0][\"id\"])\n",
    "\n",
    "#     for tweet in training_text:\n",
    "#         tweet_id = int(tweet[\"id\"])\n",
    "#         tweet_text = tweet[\"text\"]\n",
    "        \n",
    "#         row = df_ASAD.loc[df_ASAD[\"Tweet_id\"] == tweet_id]\n",
    "#         df_ASAD[\"text\"][row.index] = tweet_text\n",
    "\n",
    "\n",
    "    #\n",
    "else:\n",
    "    df_ASAD = pd.read_csv(\"data/ASAD.csv\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a766ab71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1221883443467952128</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Ø¹Ø¯Ù„Øª Ø´Ø¹Ø±ÙŠ ÙˆØ·Ù„Ø¹ ÙŠÙ‡Ø¨Ù„ ÙˆØ±Ø¨ÙŠ Ù„Ùˆ ØµØ­ÙŠØª Ø¨ÙƒØ±Ù‡ ÙˆÙ‡Ùˆ Ù…Ùˆ Ùƒ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1221875106206638080</td>\n",
       "      <td>Positive</td>\n",
       "      <td>@nas_alharbi8 ÙˆØ§Ù„Ù„Ù‡ Ø­Ø³Ø¨ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø³ÙŠÙƒÙˆÙ† Ù…Ø®ÙŠØ¨ Ù„Ù„Ø¢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1221884257490042887</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>\"Ø§Ù„Ø²Ø¹Ù„ Ø¨ÙŠØºÙŠØ± Ù…Ù„Ø§Ù…Ø­Ùƒ ØŒ Ø¨ÙŠØºÙŠØ± Ù†Ø¸Ø±Ø© Ø§Ù„Ø¹ÙŠÙ† ØŒ Ø¨ÙŠØºÙŠØ±...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1221882556548816896</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>@be4after @Fahd_Alshelaimi ØªÙˆÙ†ÙŠ Ø§Ø¯Ø±ÙŠ Ø§Ù† ÙÙŠ Ù‚Ù†Ø§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1227326811652026368</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ Ø¨Ø¹Ø¯ Ù¢Ù¡ Ø­Ø¶Ù†Øª Ø§Ù…ÙŠ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ø§ ØªØ¹Ø±ÙÙˆÙ† Ø´Ù†...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1221882634663604224</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Seemni71 Ù‡Ùˆ Ø§Ø³Ø§Ø³Ø§ ÙÙŠ Ø°ÙƒÙˆØ± Ø¯ÙŠ Ø§Ù„Ø£ÙŠØ§Ù… ğŸ˜­ğŸ’”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1221884236996775942</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Ø§Ù†Ø§ Ø¹Ø§Ø±Ù Ø§Ù† Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ø¹Ø§Ù…Ø© ÙˆØ§Ù† Ø§Ù„ÙÙ† ÙÙŠ Ù…ØµØ± Ø¨Ù‚ÙŠ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1223405995336044545</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Ø§Ù„ØªÙ‘ÙØ§Ø¤Ù„ Ù‡ÙÙˆ Ø§Ù„Ù…ÙŠØ²Ù‘Ø© Ø§Ù„Ø£ÙƒØ«ÙØ± Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ Ø¨Ø§Ù„Ù†Ù‘Ø¬Ø§Ø­...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1221880773579550720</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Ø¹Ù…Ø±ÙŠ Ù…Ø§Ø´ÙØª Ø£Ø­Ø¯ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø¨Ù‡Ø§Ø¡ ÙˆØ§Ù„Ø¹Ø°ÙˆØ¨Ù‡ ÙˆØ§Ù„Ø¥Ø´Ø±Ø§Ù‚ Ùˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1221882629684912128</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Ù„ÙƒÙŠ ØªØ±ØªØ§Ø­ ÙÙŠ Ø­ÙŠØ§ØªÙƒ ØªØ¹Ù„Ù… Ø£Ù† ØªØ®ØªØµØ± ÙÙŠ ÙƒÙ„ Ø´ÙŠØ¡ .. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id sentiment  \\\n",
       "0   1221883443467952128  Positive   \n",
       "1   1221875106206638080  Positive   \n",
       "2   1221884257490042887   Neutral   \n",
       "3   1221882556548816896   Neutral   \n",
       "4   1227326811652026368  Positive   \n",
       "..                  ...       ...   \n",
       "65  1221882634663604224  Negative   \n",
       "66  1221884236996775942  Negative   \n",
       "67  1223405995336044545   Neutral   \n",
       "68  1221880773579550720  Positive   \n",
       "69  1221882629684912128   Neutral   \n",
       "\n",
       "                                                 text  \n",
       "0   Ø¹Ø¯Ù„Øª Ø´Ø¹Ø±ÙŠ ÙˆØ·Ù„Ø¹ ÙŠÙ‡Ø¨Ù„ ÙˆØ±Ø¨ÙŠ Ù„Ùˆ ØµØ­ÙŠØª Ø¨ÙƒØ±Ù‡ ÙˆÙ‡Ùˆ Ù…Ùˆ Ùƒ...  \n",
       "1   @nas_alharbi8 ÙˆØ§Ù„Ù„Ù‡ Ø­Ø³Ø¨ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø³ÙŠÙƒÙˆÙ† Ù…Ø®ÙŠØ¨ Ù„Ù„Ø¢...  \n",
       "2   \"Ø§Ù„Ø²Ø¹Ù„ Ø¨ÙŠØºÙŠØ± Ù…Ù„Ø§Ù…Ø­Ùƒ ØŒ Ø¨ÙŠØºÙŠØ± Ù†Ø¸Ø±Ø© Ø§Ù„Ø¹ÙŠÙ† ØŒ Ø¨ÙŠØºÙŠØ±...  \n",
       "3   @be4after @Fahd_Alshelaimi ØªÙˆÙ†ÙŠ Ø§Ø¯Ø±ÙŠ Ø§Ù† ÙÙŠ Ù‚Ù†Ø§...  \n",
       "4   Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ Ø¨Ø¹Ø¯ Ù¢Ù¡ Ø­Ø¶Ù†Øª Ø§Ù…ÙŠ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ø§ ØªØ¹Ø±ÙÙˆÙ† Ø´Ù†...  \n",
       "..                                                ...  \n",
       "65            @Seemni71 Ù‡Ùˆ Ø§Ø³Ø§Ø³Ø§ ÙÙŠ Ø°ÙƒÙˆØ± Ø¯ÙŠ Ø§Ù„Ø£ÙŠØ§Ù… ğŸ˜­ğŸ’”  \n",
       "66  Ø§Ù†Ø§ Ø¹Ø§Ø±Ù Ø§Ù† Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ø¹Ø§Ù…Ø© ÙˆØ§Ù† Ø§Ù„ÙÙ† ÙÙŠ Ù…ØµØ± Ø¨Ù‚ÙŠ...  \n",
       "67  Ø§Ù„ØªÙ‘ÙØ§Ø¤Ù„ Ù‡ÙÙˆ Ø§Ù„Ù…ÙŠØ²Ù‘Ø© Ø§Ù„Ø£ÙƒØ«ÙØ± Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ Ø¨Ø§Ù„Ù†Ù‘Ø¬Ø§Ø­...  \n",
       "68  Ø¹Ù…Ø±ÙŠ Ù…Ø§Ø´ÙØª Ø£Ø­Ø¯ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø¨Ù‡Ø§Ø¡ ÙˆØ§Ù„Ø¹Ø°ÙˆØ¨Ù‡ ÙˆØ§Ù„Ø¥Ø´Ø±Ø§Ù‚ Ùˆ...  \n",
       "69  Ù„ÙƒÙŠ ØªØ±ØªØ§Ø­ ÙÙŠ Ø­ÙŠØ§ØªÙƒ ØªØ¹Ù„Ù… Ø£Ù† ØªØ®ØªØµØ± ÙÙŠ ÙƒÙ„ Ø´ÙŠØ¡ .. ...  \n",
       "\n",
       "[70 rows x 3 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ASAD.head(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "60c39d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAW_DATA:\n",
    "    df_ASAD = df_ASAD.drop([\"id\"], axis=1)\n",
    "    df_ASAD.to_csv(\"data/ASAD.csv\", encoding=\"utf-8\", index = False)\n",
    "\n",
    "df_ASAD.columns = [\"label\", \"text\"] ## dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3c3b23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ASAD[\"text\"].apply(str)\n",
    "df_ASAD[\"text\"].dropna(axis=0, how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc7620a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "8f701c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name,\n",
    "        train,\n",
    "        test,\n",
    "        label_list,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.label_list = label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "18e22ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ASAD, test_ASAD = train_test_split(df_ASAD, test_size=0.2, random_state=42)\n",
    "ASAD = Dataset(\"ASAD\", train_ASAD, test_ASAD, [\"Negative\", \"Neutral\", \"Positive\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "da5a49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arabert.preprocess import ArabertPreprocessor\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer , TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.utils import resample\n",
    "import logging\n",
    "import torch\n",
    "import optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "cf9a36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "6080fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ASAD'\n",
    "model_name = 'aubmindlab/bert-base-arabertv02'\n",
    "task_name = 'classification'\n",
    "max_len = 128\n",
    "DATA_COLUMN = \"text\"\n",
    "LABEL_COLUMN = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "3ce7adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabert_prep = ArabertPreprocessor(model_name.split(\"/\")[-1], keep_emojis=True)\n",
    "\n",
    "ASAD.train[DATA_COLUMN] = ASAD.train[DATA_COLUMN].apply(lambda x:   arabert_prep.preprocess(str(x)))\n",
    "ASAD.test[DATA_COLUMN] = ASAD.test[DATA_COLUMN].apply(lambda x:   arabert_prep.preprocess(str(x)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7510a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason, the preprocess deletes some rows in ASAD.train and ASAD.test which leads to index 0 existing, while index 1 doesn't exist... that's why we need to reset index.\n",
    "\n",
    "# ASAD.train = ASAD.train[DATA_COLUMN]\n",
    "# ASAD.test = ASAD.test[DATA_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "5009d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, text, target, model_name, max_len, label_map):\n",
    "      super(BERTDataset).__init__()\n",
    "      self.text = text\n",
    "      self.target = target\n",
    "      self.tokenizer_name = model_name\n",
    "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "      self.max_len = max_len\n",
    "      self.label_map = label_map\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.text)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "      text = str(self.text[item])\n",
    "      text = \" \".join(text.split())\n",
    "\n",
    "        \n",
    "      input_ids = self.tokenizer.encode(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          truncation='longest_first'\n",
    "      )     \n",
    "    \n",
    "      attention_mask = [1] * len(input_ids)\n",
    "\n",
    "      # Zero-pad up to the sequence length.\n",
    "      padding_length = self.max_len - len(input_ids)\n",
    "      input_ids = input_ids + ([self.tokenizer.pad_token_id] * padding_length)\n",
    "      attention_mask = attention_mask + ([0] * padding_length)    \n",
    "      \n",
    "      return InputFeatures(input_ids=input_ids, attention_mask=attention_mask, label=self.label_map[self.target[item]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "e70b7932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Negative': 0, 'Neutral': 1, 'Positive': 2}\n"
     ]
    }
   ],
   "source": [
    "label_map = { v:(index) for index, v in enumerate(ASAD.label_list) }\n",
    "print(label_map)\n",
    "train_dataset = BERTDataset(ASAD.train[DATA_COLUMN].to_list(), ASAD.train[LABEL_COLUMN].to_list(),model_name,max_len,label_map)\n",
    "test_dataset = BERTDataset(ASAD.test[DATA_COLUMN].to_list(), ASAD.test[LABEL_COLUMN].to_list(),model_name,max_len,label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "c256ce9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputFeatures(input_ids=[2, 7, 1624, 10064, 17838, 647, 20054, 36001, 58033, 323, 1848, 22929, 6412, 1769, 193, 39491, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n"
     ]
    }
   ],
   "source": [
    "# with open(\"output\", 'a') as f:\n",
    "#     for t in train_dataset.text:\n",
    "#         f.write(t)\n",
    "\n",
    "print(train_dataset[39325])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "63561f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "f224e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p): #p should be of type EvalPrediction\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  assert len(preds) == len(p.label_ids)\n",
    "  #print(classification_report(p.label_ids,preds))\n",
    "  #print(confusion_matrix(p.label_ids,preds))\n",
    "\n",
    "  macro_f1_pos_neg = f1_score(p.label_ids,preds,average='macro',labels=[0,1])\n",
    "  macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
    "  macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
    "  macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
    "  acc = accuracy_score(p.label_ids,preds)\n",
    "  return {\n",
    "      'macro_f1' : macro_f1,\n",
    "      'macro_f1_pos_neg' : macro_f1_pos_neg,  \n",
    "      'macro_precision': macro_precision,\n",
    "      'macro_recall': macro_recall,\n",
    "      'accuracy': acc\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "95ad6e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1375\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"./train\")\n",
    "training_args.evaluate_during_training = True\n",
    "training_args.adam_epsilon = 1e-8\n",
    "training_args.learning_rate = 5e-5\n",
    "training_args.fp16 = True\n",
    "training_args.per_device_train_batch_size = 16\n",
    "training_args.per_device_eval_batch_size = 16\n",
    "training_args.gradient_accumulation_steps = 2\n",
    "training_args.num_train_epochs= 8\n",
    "\n",
    "steps_per_epoch = (len(train_dataset)// (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps))\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "print(steps_per_epoch)\n",
    "print(total_steps)\n",
    "#Warmup_ratio\n",
    "warmup_ratio = 0.1\n",
    "training_args.warmup_steps = total_steps*warmup_ratio # or you can set the warmup steps directly \n",
    "\n",
    "training_args.evaluation_strategy = EvaluationStrategy.EPOCH\n",
    "# training_args.logging_steps = 200\n",
    "training_args.save_steps = 100000 #don't want to save any model, there is probably a better way to do this :)\n",
    "training_args.seed = 42\n",
    "training_args.disable_tqdm = False\n",
    "training_args.lr_scheduler_type = 'cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "f05eb4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model_init(),\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c921f228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 28:46, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro F1 Pos Neg</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.492400</td>\n",
       "      <td>0.448363</td>\n",
       "      <td>0.735160</td>\n",
       "      <td>0.748426</td>\n",
       "      <td>0.734077</td>\n",
       "      <td>0.736601</td>\n",
       "      <td>0.811545</td>\n",
       "      <td>13.033100</td>\n",
       "      <td>844.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.407200</td>\n",
       "      <td>0.464508</td>\n",
       "      <td>0.735092</td>\n",
       "      <td>0.745772</td>\n",
       "      <td>0.755890</td>\n",
       "      <td>0.728247</td>\n",
       "      <td>0.816545</td>\n",
       "      <td>13.636100</td>\n",
       "      <td>806.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.285800</td>\n",
       "      <td>0.507554</td>\n",
       "      <td>0.732670</td>\n",
       "      <td>0.747775</td>\n",
       "      <td>0.730983</td>\n",
       "      <td>0.735928</td>\n",
       "      <td>0.807455</td>\n",
       "      <td>13.639800</td>\n",
       "      <td>806.464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.177000</td>\n",
       "      <td>0.690953</td>\n",
       "      <td>0.727013</td>\n",
       "      <td>0.741517</td>\n",
       "      <td>0.713968</td>\n",
       "      <td>0.744236</td>\n",
       "      <td>0.796273</td>\n",
       "      <td>13.637900</td>\n",
       "      <td>806.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.108800</td>\n",
       "      <td>0.916192</td>\n",
       "      <td>0.723312</td>\n",
       "      <td>0.742304</td>\n",
       "      <td>0.713302</td>\n",
       "      <td>0.735550</td>\n",
       "      <td>0.794545</td>\n",
       "      <td>13.630800</td>\n",
       "      <td>806.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>1.002950</td>\n",
       "      <td>0.722534</td>\n",
       "      <td>0.745260</td>\n",
       "      <td>0.724284</td>\n",
       "      <td>0.721135</td>\n",
       "      <td>0.797273</td>\n",
       "      <td>13.665500</td>\n",
       "      <td>804.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>1.143134</td>\n",
       "      <td>0.720669</td>\n",
       "      <td>0.739196</td>\n",
       "      <td>0.723192</td>\n",
       "      <td>0.718368</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>13.631900</td>\n",
       "      <td>806.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>1.179857</td>\n",
       "      <td>0.723084</td>\n",
       "      <td>0.741847</td>\n",
       "      <td>0.725334</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.799636</td>\n",
       "      <td>13.691700</td>\n",
       "      <td>803.409000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11000, training_loss=0.20801400791515004, metrics={'train_runtime': 1726.7692, 'train_samples_per_second': 6.37, 'total_flos': 36548251508736000, 'epoch': 8.0})"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87b8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "209cea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "e8c57f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init()\n",
    "model.load_state_dict(torch.load(\"./models/pytorch_model.bin\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b0e1c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/test1_with_text.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0d81b07baca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m df_test = pd.read_csv(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m\"data/test1_with_text.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweet_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ASAD-C/env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ASAD-C/env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ASAD-C/env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ASAD-C/env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             )\n\u001b[1;32m   1044\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ASAD-C/env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1863\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ASAD-C/env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m         )\n\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ASAD-C/env/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    642\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m             )\n\u001b[1;32m    646\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/test1_with_text.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv(\n",
    "    \"data/test1_with_text.csv\", header=0\n",
    ")\n",
    "df_test = df_test[['Tweet_id','Text']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "d9573c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTestDataset(Dataset):\n",
    "    def __init__(self, text, target, model_name, max_len, label_map):\n",
    "      super(BERTDataset).__init__()\n",
    "      self.text = text\n",
    "      self.target = target\n",
    "      self.tokenizer_name = model_name\n",
    "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "      self.max_len = max_len\n",
    "      self.label_map = label_map\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.text)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "      text = str(self.text[item])\n",
    "      text = \" \".join(text.split())\n",
    "\n",
    "        \n",
    "      input_ids = self.tokenizer.encode(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          truncation='longest_first'\n",
    "      )     \n",
    "    \n",
    "      attention_mask = [1] * len(input_ids)\n",
    "\n",
    "      # Zero-pad up to the sequence length.\n",
    "      padding_length = self.max_len - len(input_ids)\n",
    "      input_ids = input_ids + ([self.tokenizer.pad_token_id] * padding_length)\n",
    "      attention_mask = attention_mask + ([0] * padding_length)    \n",
    "      \n",
    "      return input_ids=input_ids, attention_mask=attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "c68a749e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SequentialSampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-310-eb92dc4ac135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbert_test_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTTestDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# test_dataset = TensorDataset(test_inputs, test_masks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_test_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_test_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SequentialSampler' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "cf9d32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "\n",
    "trainer.predict()\n",
    "\n",
    "bert_test_dataset = BERTTestDataset(df_test[\"Text\"], [], model_name, max_len, [])\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(bert_test_dataset)\n",
    "test_dataloader = DataLoader(bert_test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "acd7e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "56cca2d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'transformers.data.processors.utils.InputFeatures'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-317-6ea25dd0b974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-316-d00913972a14>\u001b[0m in \u001b[0;36mbert_predict\u001b[0;34m(model, test_dataloader)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# For each batch in our test set...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Load batch to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_attn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ASAD-C/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ASAD-C/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ASAD-C/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ASAD-C/env/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'transformers.data.processors.utils.InputFeatures'>"
     ]
    }
   ],
   "source": [
    "preds = bert_predict(model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c310a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
