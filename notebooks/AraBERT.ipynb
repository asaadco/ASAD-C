{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9df252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n",
      "Sat Apr 24 23:10:59 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.119.03   Driver Version: 450.119.03   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN RTX           Off  | 00000000:18:00.0 Off |                  N/A |\n",
      "| 40%   27C    P8    15W / 280W |      3MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN RTX           Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 41%   30C    P8     2W / 280W |      0MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN RTX           Off  | 00000000:86:00.0 Off |                  N/A |\n",
      "| 40%   28C    P8    14W / 280W |      0MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    !nvidia-smi\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce0dc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: optuna==2.3.0 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: cmaes>=0.6.0 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from optuna==2.3.0) (0.8.2)\n",
      "Requirement already satisfied: alembic in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from optuna==2.3.0) (1.5.8)\n",
      "Requirement already satisfied: colorlog in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from optuna==2.3.0) (5.0.1)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from optuna==2.3.0) (1.6.1)\n",
      "Requirement already satisfied: joblib in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from optuna==2.3.0) (1.0.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from optuna==2.3.0) (1.4.9)\n",
      "Requirement already satisfied: numpy in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from optuna==2.3.0) (1.20.1)\n",
      "Requirement already satisfied: cliff in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from optuna==2.3.0) (3.7.0)\n",
      "Requirement already satisfied: tqdm in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from optuna==2.3.0) (4.60.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from optuna==2.3.0) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from packaging>=20.0->optuna==2.3.0) (2.4.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna==2.3.0) (3.10.1)\n",
      "Requirement already satisfied: python-dateutil in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from alembic->optuna==2.3.0) (2.8.1)\n",
      "Requirement already satisfied: python-editor>=0.3 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from alembic->optuna==2.3.0) (1.0.4)\n",
      "Requirement already satisfied: Mako in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from alembic->optuna==2.3.0) (1.1.4)\n",
      "Requirement already satisfied: PyYAML>=3.12 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from cliff->optuna==2.3.0) (5.3.1)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from cliff->optuna==2.3.0) (1.5.0)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from cliff->optuna==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from cliff->optuna==2.3.0) (5.5.1)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from cliff->optuna==2.3.0) (2.1.0)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (20.3.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (0.2.5)\n",
      "Requirement already satisfied: colorama>=0.3.7 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (0.4.4)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==2.3.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==2.3.0) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from Mako->alembic->optuna==2.3.0) (1.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from python-dateutil->alembic->optuna==2.3.0) (1.15.0)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: transformers==4.2.1 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (4.2.1)\n",
      "Requirement already satisfied: packaging in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from transformers==4.2.1) (20.9)\n",
      "Requirement already satisfied: requests in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from transformers==4.2.1) (2.25.1)\n",
      "Requirement already satisfied: numpy in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from transformers==4.2.1) (1.20.1)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from transformers==4.2.1) (0.9.4)\n",
      "Requirement already satisfied: sacremoses in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from transformers==4.2.1) (0.0.45)\n",
      "Requirement already satisfied: importlib-metadata in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from transformers==4.2.1) (3.10.1)\n",
      "Requirement already satisfied: filelock in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from transformers==4.2.1) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from transformers==4.2.1) (2021.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from transformers==4.2.1) (4.60.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from importlib-metadata->transformers==4.2.1) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from importlib-metadata->transformers==4.2.1) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from packaging->transformers==4.2.1) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from requests->transformers==4.2.1) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from requests->transformers==4.2.1) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from requests->transformers==4.2.1) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from requests->transformers==4.2.1) (2020.12.5)\n",
      "Requirement already satisfied: click in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from sacremoses->transformers==4.2.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from sacremoses->transformers==4.2.1) (1.0.1)\n",
      "Requirement already satisfied: six in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from sacremoses->transformers==4.2.1) (1.15.0)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: farasapy in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (0.0.13)\n",
      "Requirement already satisfied: requests in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from farasapy) (2.25.1)\n",
      "Requirement already satisfied: tqdm in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from farasapy) (4.60.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from requests->farasapy) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from requests->farasapy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from requests->farasapy) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (from requests->farasapy) (2020.12.5)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: pyarabic in /home/alghas0c/ASAD-C/env/lib/python3.7/site-packages (0.6.10)\n",
      "fatal: destination path 'arabert' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna==2.3.0\n",
    "!pip install transformers==4.2.1\n",
    "!pip install farasapy\n",
    "!pip install pyarabic\n",
    "!git clone https://github.com/aub-mind/arabert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00249a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "os.chdir(\"/home/alghas0c/ASAD-C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "757d4a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# TO-DO: Add ASAD Dataset\n",
    "import json\n",
    "RAW_DATA = True\n",
    "pd.options.mode.chained_assignment = None \n",
    "if RAW_DATA:\n",
    "    df_ASAD = pd.read_csv(\n",
    "        \"data/training_file.csv\", header=0\n",
    "    )\n",
    "    df_ASAD = df_ASAD[['Tweet_id','sentiment']]\n",
    "    df_ASAD.rename(columns = {'Tweet_id':'id', 'sentiment':'sentiment'}, inplace=True)\n",
    "   \n",
    "    # Replace all the tweet ids with text\n",
    "        # Read from JSON file\n",
    "    with open(\"data/training_file.json\") as jsonFile:\n",
    "        training_text = json.load(jsonFile)\n",
    "    textframe = pd.DataFrame.from_dict(training_text)\n",
    "    df_ASAD[\"sentiment\"]=df_ASAD[\"sentiment\"].astype(str)\n",
    "    df_ASAD[\"id\"] = df_ASAD[\"id\"].astype(str)\n",
    "    textframe[\"text\"] = textframe[\"text\"].astype(str)\n",
    "    textframe[\"id\"] = textframe[\"id\"].astype(str)\n",
    "    print(df_ASAD.columns)\n",
    "#     print(len(textframe[[\"id\"].unique() == len(textframe[\"id\"]))\n",
    "    df_ASAD = df_ASAD.merge(textframe, on = 'id', how=\"left\")\n",
    "#     df_ASAD[\"text\"] = textframe[textframe.id == df_ASAD.id][\"text\"]\n",
    "    #     print(training_text[0][\"id\"])\n",
    "\n",
    "#     for tweet in training_text:\n",
    "#         tweet_id = int(tweet[\"id\"])\n",
    "#         tweet_text = tweet[\"text\"]\n",
    "        \n",
    "#         row = df_ASAD.loc[df_ASAD[\"Tweet_id\"] == tweet_id]\n",
    "#         df_ASAD[\"text\"][row.index] = tweet_text\n",
    "\n",
    "\n",
    "    #\n",
    "else:\n",
    "    df_ASAD = pd.read_csv(\"data/ASAD.csv\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0bfee61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1221883443467952128</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Ø¹Ø¯Ù„Øª Ø´Ø¹Ø±ÙŠ ÙˆØ·Ù„Ø¹ ÙŠÙ‡Ø¨Ù„ ÙˆØ±Ø¨ÙŠ Ù„Ùˆ ØµØ­ÙŠØª Ø¨ÙƒØ±Ù‡ ÙˆÙ‡Ùˆ Ù…Ùˆ Ùƒ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1221875106206638080</td>\n",
       "      <td>Positive</td>\n",
       "      <td>@nas_alharbi8 ÙˆØ§Ù„Ù„Ù‡ Ø­Ø³Ø¨ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø³ÙŠÙƒÙˆÙ† Ù…Ø®ÙŠØ¨ Ù„Ù„Ø¢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1221884257490042887</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>\"Ø§Ù„Ø²Ø¹Ù„ Ø¨ÙŠØºÙŠØ± Ù…Ù„Ø§Ù…Ø­Ùƒ ØŒ Ø¨ÙŠØºÙŠØ± Ù†Ø¸Ø±Ø© Ø§Ù„Ø¹ÙŠÙ† ØŒ Ø¨ÙŠØºÙŠØ±...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1221882556548816896</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>@be4after @Fahd_Alshelaimi ØªÙˆÙ†ÙŠ Ø§Ø¯Ø±ÙŠ Ø§Ù† ÙÙŠ Ù‚Ù†Ø§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1227326811652026368</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ Ø¨Ø¹Ø¯ Ù¢Ù¡ Ø­Ø¶Ù†Øª Ø§Ù…ÙŠ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ø§ ØªØ¹Ø±ÙÙˆÙ† Ø´Ù†...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1221882634663604224</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Seemni71 Ù‡Ùˆ Ø§Ø³Ø§Ø³Ø§ ÙÙŠ Ø°ÙƒÙˆØ± Ø¯ÙŠ Ø§Ù„Ø£ÙŠØ§Ù… ğŸ˜­ğŸ’”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1221884236996775942</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Ø§Ù†Ø§ Ø¹Ø§Ø±Ù Ø§Ù† Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ø¹Ø§Ù…Ø© ÙˆØ§Ù† Ø§Ù„ÙÙ† ÙÙŠ Ù…ØµØ± Ø¨Ù‚ÙŠ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1223405995336044545</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Ø§Ù„ØªÙ‘ÙØ§Ø¤Ù„ Ù‡ÙÙˆ Ø§Ù„Ù…ÙŠØ²Ù‘Ø© Ø§Ù„Ø£ÙƒØ«ÙØ± Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ Ø¨Ø§Ù„Ù†Ù‘Ø¬Ø§Ø­...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1221880773579550720</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Ø¹Ù…Ø±ÙŠ Ù…Ø§Ø´ÙØª Ø£Ø­Ø¯ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø¨Ù‡Ø§Ø¡ ÙˆØ§Ù„Ø¹Ø°ÙˆØ¨Ù‡ ÙˆØ§Ù„Ø¥Ø´Ø±Ø§Ù‚ Ùˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1221882629684912128</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Ù„ÙƒÙŠ ØªØ±ØªØ§Ø­ ÙÙŠ Ø­ÙŠØ§ØªÙƒ ØªØ¹Ù„Ù… Ø£Ù† ØªØ®ØªØµØ± ÙÙŠ ÙƒÙ„ Ø´ÙŠØ¡ .. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id sentiment  \\\n",
       "0   1221883443467952128  Positive   \n",
       "1   1221875106206638080  Positive   \n",
       "2   1221884257490042887   Neutral   \n",
       "3   1221882556548816896   Neutral   \n",
       "4   1227326811652026368  Positive   \n",
       "..                  ...       ...   \n",
       "65  1221882634663604224  Negative   \n",
       "66  1221884236996775942  Negative   \n",
       "67  1223405995336044545   Neutral   \n",
       "68  1221880773579550720  Positive   \n",
       "69  1221882629684912128   Neutral   \n",
       "\n",
       "                                                 text  \n",
       "0   Ø¹Ø¯Ù„Øª Ø´Ø¹Ø±ÙŠ ÙˆØ·Ù„Ø¹ ÙŠÙ‡Ø¨Ù„ ÙˆØ±Ø¨ÙŠ Ù„Ùˆ ØµØ­ÙŠØª Ø¨ÙƒØ±Ù‡ ÙˆÙ‡Ùˆ Ù…Ùˆ Ùƒ...  \n",
       "1   @nas_alharbi8 ÙˆØ§Ù„Ù„Ù‡ Ø­Ø³Ø¨ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø³ÙŠÙƒÙˆÙ† Ù…Ø®ÙŠØ¨ Ù„Ù„Ø¢...  \n",
       "2   \"Ø§Ù„Ø²Ø¹Ù„ Ø¨ÙŠØºÙŠØ± Ù…Ù„Ø§Ù…Ø­Ùƒ ØŒ Ø¨ÙŠØºÙŠØ± Ù†Ø¸Ø±Ø© Ø§Ù„Ø¹ÙŠÙ† ØŒ Ø¨ÙŠØºÙŠØ±...  \n",
       "3   @be4after @Fahd_Alshelaimi ØªÙˆÙ†ÙŠ Ø§Ø¯Ø±ÙŠ Ø§Ù† ÙÙŠ Ù‚Ù†Ø§...  \n",
       "4   Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ Ø¨Ø¹Ø¯ Ù¢Ù¡ Ø­Ø¶Ù†Øª Ø§Ù…ÙŠ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ø§ ØªØ¹Ø±ÙÙˆÙ† Ø´Ù†...  \n",
       "..                                                ...  \n",
       "65            @Seemni71 Ù‡Ùˆ Ø§Ø³Ø§Ø³Ø§ ÙÙŠ Ø°ÙƒÙˆØ± Ø¯ÙŠ Ø§Ù„Ø£ÙŠØ§Ù… ğŸ˜­ğŸ’”  \n",
       "66  Ø§Ù†Ø§ Ø¹Ø§Ø±Ù Ø§Ù† Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ø¹Ø§Ù…Ø© ÙˆØ§Ù† Ø§Ù„ÙÙ† ÙÙŠ Ù…ØµØ± Ø¨Ù‚ÙŠ...  \n",
       "67  Ø§Ù„ØªÙ‘ÙØ§Ø¤Ù„ Ù‡ÙÙˆ Ø§Ù„Ù…ÙŠØ²Ù‘Ø© Ø§Ù„Ø£ÙƒØ«ÙØ± Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ Ø¨Ø§Ù„Ù†Ù‘Ø¬Ø§Ø­...  \n",
       "68  Ø¹Ù…Ø±ÙŠ Ù…Ø§Ø´ÙØª Ø£Ø­Ø¯ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø¨Ù‡Ø§Ø¡ ÙˆØ§Ù„Ø¹Ø°ÙˆØ¨Ù‡ ÙˆØ§Ù„Ø¥Ø´Ø±Ø§Ù‚ Ùˆ...  \n",
       "69  Ù„ÙƒÙŠ ØªØ±ØªØ§Ø­ ÙÙŠ Ø­ÙŠØ§ØªÙƒ ØªØ¹Ù„Ù… Ø£Ù† ØªØ®ØªØµØ± ÙÙŠ ÙƒÙ„ Ø´ÙŠØ¡ .. ...  \n",
       "\n",
       "[70 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ASAD.head(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc524feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAW_DATA:\n",
    "    df_ASAD = df_ASAD.drop([\"id\"], axis=1)\n",
    "    df_ASAD.to_csv(\"data/ASAD.csv\", encoding=\"utf-8\", index = False)\n",
    "\n",
    "df_ASAD.columns = [\"label\", \"text\"] ## dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716360f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ASAD[\"text\"].apply(str)\n",
    "df_ASAD[\"text\"].dropna(axis=0, how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d8fc99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334b365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name,\n",
    "        train,\n",
    "        test,\n",
    "        label_list,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.label_list = label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e977a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ASAD, test_ASAD = train_test_split(df_ASAD, test_size=0.2, random_state=42)\n",
    "ASAD = Dataset(\"ASAD\", train_ASAD, test_ASAD, [\"Negative\", \"Neutral\", \"Positive\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d277633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arabert.preprocess import ArabertPreprocessor\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer , TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.utils import resample\n",
    "import logging\n",
    "import torch\n",
    "import optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4ef63f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d618871",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ASAD'\n",
    "model_name = 'aubmindlab/bert-base-arabertv02'\n",
    "task_name = 'classification'\n",
    "max_len = 128\n",
    "DATA_COLUMN = \"text\"\n",
    "LABEL_COLUMN = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a53fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabert_prep = ArabertPreprocessor(model_name.split(\"/\")[-1], keep_emojis=True)\n",
    "\n",
    "ASAD.train[DATA_COLUMN] = ASAD.train[DATA_COLUMN].apply(lambda x:   arabert_prep.preprocess(str(x)))\n",
    "ASAD.test[DATA_COLUMN] = ASAD.test[DATA_COLUMN].apply(lambda x:   arabert_prep.preprocess(str(x)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason, the preprocess deletes some rows in ASAD.train and ASAD.test which leads to index 0 existing, while index 1 doesn't exist... that's why we need to reset index.\n",
    "\n",
    "# ASAD.train = ASAD.train[DATA_COLUMN]\n",
    "# ASAD.test = ASAD.test[DATA_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "182f7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, text, target, model_name, max_len, label_map):\n",
    "      super(BERTDataset).__init__()\n",
    "      self.text = text\n",
    "      self.target = target\n",
    "      self.tokenizer_name = model_name\n",
    "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "      self.max_len = max_len\n",
    "      self.label_map = label_map\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.text)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "      text = str(self.text[item])\n",
    "      text = \" \".join(text.split())\n",
    "\n",
    "        \n",
    "      input_ids = self.tokenizer.encode(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          truncation='longest_first'\n",
    "      )     \n",
    "    \n",
    "      attention_mask = [1] * len(input_ids)\n",
    "\n",
    "      # Zero-pad up to the sequence length.\n",
    "      padding_length = self.max_len - len(input_ids)\n",
    "      input_ids = input_ids + ([self.tokenizer.pad_token_id] * padding_length)\n",
    "      attention_mask = attention_mask + ([0] * padding_length)    \n",
    "      \n",
    "      return InputFeatures(input_ids=input_ids, attention_mask=attention_mask, label=self.label_map[self.target[item]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ae1b800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Negative': 0, 'Neutral': 1, 'Positive': 2}\n"
     ]
    }
   ],
   "source": [
    "label_map = { v:(index) for index, v in enumerate(ASAD.label_list) }\n",
    "print(label_map)\n",
    "train_dataset = BERTDataset(ASAD.train[DATA_COLUMN].to_list(), ASAD.train[LABEL_COLUMN].to_list(),model_name,max_len,label_map)\n",
    "test_dataset = BERTDataset(ASAD.test[DATA_COLUMN].to_list(), ASAD.test[LABEL_COLUMN].to_list(),model_name,max_len,label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74b6abde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputFeatures(input_ids=[2, 7, 1624, 10064, 17838, 647, 20054, 36001, 58033, 323, 1848, 22929, 6412, 1769, 193, 39491, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n"
     ]
    }
   ],
   "source": [
    "# with open(\"output\", 'a') as f:\n",
    "#     for t in train_dataset.text:\n",
    "#         f.write(t)\n",
    "\n",
    "print(train_dataset[39325])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "654d9f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7e15a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p): #p should be of type EvalPrediction\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  assert len(preds) == len(p.label_ids)\n",
    "  #print(classification_report(p.label_ids,preds))\n",
    "  #print(confusion_matrix(p.label_ids,preds))\n",
    "\n",
    "  macro_f1_pos_neg = f1_score(p.label_ids,preds,average='macro',labels=[0,1])\n",
    "  macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
    "  macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
    "  macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
    "  acc = accuracy_score(p.label_ids,preds)\n",
    "  return {\n",
    "      'macro_f1' : macro_f1,\n",
    "      'macro_f1_pos_neg' : macro_f1_pos_neg,  \n",
    "      'macro_precision': macro_precision,\n",
    "      'macro_recall': macro_recall,\n",
    "      'accuracy': acc\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3551d7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1375\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"./train\")\n",
    "training_args.evaluate_during_training = True\n",
    "training_args.adam_epsilon = 1e-8\n",
    "training_args.learning_rate = 5e-5\n",
    "training_args.fp16 = True\n",
    "training_args.per_device_train_batch_size = 16\n",
    "training_args.per_device_eval_batch_size = 16\n",
    "training_args.gradient_accumulation_steps = 2\n",
    "training_args.num_train_epochs= 8\n",
    "\n",
    "steps_per_epoch = (len(train_dataset)// (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps))\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "print(steps_per_epoch)\n",
    "print(total_steps)\n",
    "#Warmup_ratio\n",
    "warmup_ratio = 0.1\n",
    "training_args.warmup_steps = total_steps*warmup_ratio # or you can set the warmup steps directly \n",
    "\n",
    "training_args.evaluation_strategy = EvaluationStrategy.EPOCH\n",
    "# training_args.logging_steps = 200\n",
    "training_args.save_steps = 100000 #don't want to save any model, there is probably a better way to do this :)\n",
    "training_args.seed = 42\n",
    "training_args.disable_tqdm = False\n",
    "training_args.lr_scheduler_type = 'cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c55e224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model_init(),\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b806984c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alghas0c/ASAD-C/env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3664' max='3664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3664/3664 24:44, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro F1 Pos Neg</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.458704</td>\n",
       "      <td>0.721836</td>\n",
       "      <td>0.728222</td>\n",
       "      <td>0.741883</td>\n",
       "      <td>0.728841</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>16.607600</td>\n",
       "      <td>662.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.583700</td>\n",
       "      <td>0.429251</td>\n",
       "      <td>0.745367</td>\n",
       "      <td>0.761375</td>\n",
       "      <td>0.754511</td>\n",
       "      <td>0.738744</td>\n",
       "      <td>0.821818</td>\n",
       "      <td>16.715500</td>\n",
       "      <td>658.073000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.422800</td>\n",
       "      <td>0.458374</td>\n",
       "      <td>0.730394</td>\n",
       "      <td>0.743158</td>\n",
       "      <td>0.772924</td>\n",
       "      <td>0.703944</td>\n",
       "      <td>0.818091</td>\n",
       "      <td>17.029100</td>\n",
       "      <td>645.954000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.507995</td>\n",
       "      <td>0.733494</td>\n",
       "      <td>0.746744</td>\n",
       "      <td>0.751375</td>\n",
       "      <td>0.722868</td>\n",
       "      <td>0.814636</td>\n",
       "      <td>17.179800</td>\n",
       "      <td>640.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.680912</td>\n",
       "      <td>0.729622</td>\n",
       "      <td>0.742941</td>\n",
       "      <td>0.727154</td>\n",
       "      <td>0.734996</td>\n",
       "      <td>0.803182</td>\n",
       "      <td>17.324500</td>\n",
       "      <td>634.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.792540</td>\n",
       "      <td>0.727090</td>\n",
       "      <td>0.743416</td>\n",
       "      <td>0.732399</td>\n",
       "      <td>0.722170</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>17.283600</td>\n",
       "      <td>636.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.917245</td>\n",
       "      <td>0.725352</td>\n",
       "      <td>0.741486</td>\n",
       "      <td>0.715158</td>\n",
       "      <td>0.737781</td>\n",
       "      <td>0.793818</td>\n",
       "      <td>17.493100</td>\n",
       "      <td>628.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.953900</td>\n",
       "      <td>0.724509</td>\n",
       "      <td>0.739942</td>\n",
       "      <td>0.722104</td>\n",
       "      <td>0.727147</td>\n",
       "      <td>0.798545</td>\n",
       "      <td>17.298700</td>\n",
       "      <td>635.887000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alghas0c/ASAD-C/env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/alghas0c/ASAD-C/env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/alghas0c/ASAD-C/env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/alghas0c/ASAD-C/env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/alghas0c/ASAD-C/env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/alghas0c/ASAD-C/env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/alghas0c/ASAD-C/env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3664, training_loss=0.2508254707119871, metrics={'train_runtime': 1489.4984, 'train_samples_per_second': 2.46, 'total_flos': 36544928940417024, 'epoch': 8.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "33db15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6822bf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = model_init()\n",
    "# model.load_state_dict(torch.load(\"./models/pytorch_model.bin\"))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0544b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv(\n",
    "    \"data/test1_with_text.csv\", header=0\n",
    ")\n",
    "df_test = df_test[['Tweet_id','Text']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80dba27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTestDataset(Dataset):\n",
    "    def __init__(self, text, target, model_name, max_len, label_map):\n",
    "      super(BERTDataset).__init__()\n",
    "      self.text = text\n",
    "      self.target = target\n",
    "      self.tokenizer_name = model_name\n",
    "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "      self.max_len = max_len\n",
    "      self.label_map = label_map\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.text)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "      text = str(self.text[item])\n",
    "      text = \" \".join(text.split())\n",
    "\n",
    "        \n",
    "      input_ids = self.tokenizer.encode(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          truncation='longest_first'\n",
    "      )     \n",
    "    \n",
    "      attention_mask = [1] * len(input_ids)\n",
    "\n",
    "      # Zero-pad up to the sequence length.\n",
    "      padding_length = self.max_len - len(input_ids)\n",
    "      input_ids = input_ids + ([self.tokenizer.pad_token_id] * padding_length)\n",
    "      attention_mask = attention_mask + ([0] * padding_length)    \n",
    "      \n",
    "      return InputFeatures(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b3d0bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test_dataset = BERTTestDataset(df_test[\"Text\"], [], model_name, max_len, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b4072c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(bert_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4189fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, metrics = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3f1f7228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., -1,  1,  0])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_preds = np.array([])\n",
    "sentiment_preds = predictions.argmax(axis=1)-1\n",
    "\n",
    "\n",
    "sentiment_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "753e039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"sentiment\"] = sentiment_preds.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7223129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['Text'], axis = 1)\n",
    "df_test.to_csv(\"data/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
